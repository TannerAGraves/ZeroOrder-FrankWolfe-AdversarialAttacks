{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 10e-3\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = 64, # TEST BATCH SIZE\n",
    "                                           shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD FROM SAVED FILE\n",
    "LeNet = torch.load('LeNet.pth')\n",
    "LeNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def sfw(image_toattack, tgt_lab, model):\n",
    "    \"\"\"\n",
    "    (Note: The model parameter is added to take a PyTorch model for predictions and gradients)\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    epsilon = 0.3    \n",
    "    step = 0.4\n",
    "    batch_size = 400\n",
    "    T = 400\n",
    "\n",
    "    lmo_calls = 0\n",
    "    sfo_calls = 0\n",
    "\n",
    "    # Assuming input is numpy, convert to torch tensor\n",
    "    x_ori = torch.tensor(image_toattack, dtype=torch.float32)\n",
    "    x = x_ori.clone()\n",
    "    x_set = [x_ori]\n",
    "    all_tuples = [(i,j) for i in range(28) for j in range(28)]\n",
    "\n",
    "    for t in range(T):\n",
    "        lmo_calls += 1\n",
    "        sfo_calls += batch_size\n",
    "\n",
    "        target_reached, _ = run_classification(x, tgt_lab, model)  # Assuming run_classification is modified for PyTorch\n",
    "        if target_reached: break\n",
    "\n",
    "        zetas_ind = torch.randint(0, 28*28, (batch_size,))\n",
    "        zetas = [all_tuples[i] for i in zetas_ind]\n",
    "\n",
    "        m = torch.zeros(1,28,28,1)\n",
    "        g = evaluate_grads(x, tgt_lab, model)  # Assuming evaluate_grads is modified for PyTorch\n",
    "\n",
    "        for index,tupla in enumerate(zetas):\n",
    "            i, j = tupla\n",
    "            m[0,i,j,0] = g[0,i,j,0]\n",
    "\n",
    "        m = m * (-1/batch_size)\n",
    "        v = epsilon * torch.sign(m) + x_ori\n",
    "\n",
    "        d = v - x\n",
    "        x = x + step * d\n",
    "\n",
    "        x_set.append(x) \n",
    "\n",
    "    # Calculate distortion\n",
    "    distortion = torch.max(torch.abs(x - x_ori))\n",
    "\n",
    "    ind = np.random.choice(range(len(x_set)))\n",
    "    x_alpha = x_set[ind]\n",
    "    return x_alpha, sfo_calls, distortion, target_reached, lmo_calls\n",
    "\n",
    "def run_classification(x, tgt_lab, model):\n",
    "    \"\"\"\n",
    "    Modify this function for PyTorch.\n",
    "    \"\"\"\n",
    "    # Dummy return for now\n",
    "    return False, None\n",
    "\n",
    "def evaluate_grads(x, tgt_lab, model):\n",
    "    \"\"\"\n",
    "    Modify this function for PyTorch to compute gradients.\n",
    "    \"\"\"\n",
    "    # Dummy return for now\n",
    "    return torch.zeros_like(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "estimator = \"UniGE\"\n",
    "query_count = 0\n",
    "d = 32*32\n",
    "w = torch.zeros(d) # not sure if this goes here or somewhere else\n",
    "def compute_loss(self, x, y, w=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor) : batch data with shape [B x D].\n",
    "            y (torch.Tensor) : batch label with shape [B].\n",
    "            w (torch.Tensor) : batch w with shape [B x Q x D].\n",
    "        \"\"\"\n",
    "        c = self.sigma\n",
    "        if w is None:\n",
    "            # Batch loss computing for loss\n",
    "            w = self.w      # (D)\n",
    "            res = -(y - x @ w)**2\n",
    "            res = c * c / 2 * (1 - torch.exp(res / (c * c)))\n",
    "            return res\n",
    "        else:\n",
    "            # Batch loss computing for gradient estimating\n",
    "            pred = torch.bmm(w, x.unsqueeze(dim=2)).squeeze()       # (B, 2Q)\n",
    "            res = -(y - pred)**2                                    # (B, 2Q)\n",
    "            res = c * c / 2 * (1 - torch.exp(res / (c * c)))        # (B, 2Q)\n",
    "            return res\n",
    "        # return 1 / (1 + torch.exp(y * (x @ w)))\n",
    "        # return (y - x @ w)**2\n",
    "        return res\n",
    "\n",
    "def UniGE(x, y, mu, noise=None, w=None):\n",
    "    with torch.no_grad():\n",
    "        B = x.size(0)\n",
    "        Q = self.q\n",
    "        \n",
    "        #w = self.w if w is None else w      # (D)\n",
    "        w_left = w.unsqueeze(dim=0).unsqueeze(dim=0).repeat(B, Q, 1)        # (B, Q, D)\n",
    "        if noise is None:\n",
    "            noise = torch.rand_like(w_left) * 2 - 1     # Scaling uniform distribution from [0, 1) to [-1, 1)\n",
    "            norm = noise.view(B*Q, -1).norm(dim=-1).view(B, Q, 1)        # (B, Q, 1)\n",
    "            noise = noise / norm        # (B, Q, D)\n",
    "\n",
    "        w_all = torch.cat([w_left, w_left], dim=1)      # (B, 2Q, D)\n",
    "        noise_all = torch.cat([noise, torch.zeros_like(w_left)], dim=1)\n",
    "\n",
    "        target = y.unsqueeze(dim=1).repeat(1, 2*Q)      # (B, 2Q)\n",
    "\n",
    "        loss = compute_loss(x, target, w_all + mu * noise_all)     # (B, 2Q)\n",
    "        loss_left, loss_right = loss[:, :Q], loss[:, Q:]        # (B, Q), (B, Q)\n",
    "\n",
    "        grad = (loss_left - loss_right).view(B, Q, 1) * noise   # (B, Q, D)\n",
    "        grad = torch.mean(grad, dim=1)      # (B, D)\n",
    "        grad = torch.mean(grad, dim=0)      # (D)\n",
    "        grad = grad * (self.d / mu)\n",
    "    return grad, 2 * Q * B, noise\n",
    "\n",
    "def est_grad(x, y, mu, w_old=None):\n",
    "    # Estimate gradient using GauGE/UniGE/CooGE\n",
    "    #if self.estimator == 'GauGE':\n",
    "        #grad, query_size, noise = self.GauGE(x, y, mu)\n",
    "        #self.query_count += query_size\n",
    "        #if w_old is not None:\n",
    "        #    grad_old, query_size, _ = self.GauGE(x, y, mu, noise, w_old)\n",
    "        #    self.query_count += query_size\n",
    "    if estimator == 'UniGE':\n",
    "        grad, query_size, noise = UniGE(x, y, mu)\n",
    "        query_count += query_size\n",
    "        if w_old is not None:\n",
    "            grad_old, query_size, _ = UniGE(x, y, mu, noise, w_old)\n",
    "            query_count += query_size\n",
    "    #elif self.estimator == 'CooGE':\n",
    "        #grad, query_size = self.CooGE(x, y, mu)\n",
    "        #self.query_count += query_size\n",
    "        #if w_old is not None:\n",
    "        #    grad_old, query_size = self.CooGE(x, y, mu, w_old)\n",
    "        #    self.query_count += query_size\n",
    "    if w_old is not None:\n",
    "        return grad, grad_old\n",
    "    else:\n",
    "        return grad\n",
    "    \n",
    "def LMO_L1(grad, theta):\n",
    "    coord = torch.argmax(grad.abs())\n",
    "    v = torch.zeros_like(grad)\n",
    "    v[coord] = theta * torch.sign(grad[coord])\n",
    "    return -v\n",
    "def attack(theta = 1, T = 1000, base_lr = 1, log_step = 10):\n",
    "        with torch.no_grad():\n",
    "            batch_fetcher = DataFetcher(self.train_data, batch_size=self.batch_size)\n",
    "            \n",
    "\n",
    "            for iteration in tqdm(range(self.T)):\n",
    "                \n",
    "                if iteration % log_step == 0:\n",
    "                    # Sanity check\n",
    "                    weight_norm = w.abs().sum()\n",
    "                    is_valid = weight_norm <= (self.theta + 1e-6)\n",
    "                    if not is_valid:\n",
    "                        print('!!! ??? weight out of range ({} > {})'.format(weight_norm, theta))\n",
    "                        return 0\n",
    "                    \n",
    "                    train_loss = batch_compute_loss(self.model.compute_loss, self.train_data)\n",
    "                    test_loss = batch_compute_loss(self.model.compute_loss, self.test_data)\n",
    "\n",
    "                    desc = 'Iter {} | TrainLoss {:.6f}  | TestLoss {:.6f} | Norm {:.4f} | lr {:.6f} | beta {:.6f} | delta {:.6f} |'.format(iteration, train_loss, test_loss, weight_norm.item(), self.lr(iteration), self.beta(iteration), self.delta(iteration))\n",
    "                    tqdm.write(desc)\n",
    "                    #if writer is not None:\n",
    "                    #    writer.add_scalar(flag[0], train_loss, global_step=iteration)\n",
    "                    #    writer.add_scalar(flag[1], test_loss, global_step=iteration)\n",
    "\n",
    "                x, y = batch_fetcher.fetch()\n",
    "                grad = model.est_grad(x, y, self.delta(iteration))\n",
    "\n",
    "                if iteration == 0:\n",
    "                    m = grad\n",
    "                m = (1 - self.beta(iteration)) * m + self.beta(iteration) * grad\n",
    "                grad = m\n",
    "\n",
    "                v = LMO_L1(grad, self.theta)\n",
    "                d = v - w\n",
    "                w = w + self.lr(iteration) * d                \n",
    "\n",
    "        return train_loss, test_loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
