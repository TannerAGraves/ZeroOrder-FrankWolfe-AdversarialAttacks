{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 10e-3\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = 64, # TEST BATCH SIZE\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(LeNet.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD FROM SAVED FILE\n",
    "LeNet = torch.load('LeNet.pth')\n",
    "LeNet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://federicozanotti.github.io/2021-09-16-frank-wolfe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(batched_data):\n",
    "    b, _, d, _ = batched_data.shape\n",
    "    return batched_data.view(b, 1, d*d)\n",
    "\n",
    "def unflatten_data(flattened_data, d):\n",
    "    b, _ = flattened_data.shape[:2]\n",
    "    return flattened_data.view(b, 1, d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_torch(x, y_true, model):\n",
    "    \"\"\"\n",
    "    Loss function for all the examples\n",
    "\n",
    "    Input:\n",
    "    - x: images [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - model: the PyTorch model used for prediction\n",
    "\n",
    "    Returns:\n",
    "    - Loss: computed loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict class scores\n",
    "    f = model(x)\n",
    "    \n",
    "    # Gather the scores for the correct class\n",
    "    f_yi = torch.gather(f, 1, y_true.view(-1, 1)).squeeze()\n",
    "    \n",
    "    # Create a mask for the correct class, setting them to a very small value\n",
    "    mask = torch.arange(f.size(1)).expand(f.size(0), f.size(1)).to(y_true.device)  # This creates a matrix of size batch_size x num_classes\n",
    "    mask = mask != y_true.view(-1, 1)  # This gives a mask where the correct class for each example is set to False\n",
    "    \n",
    "    # Using the mask, set scores of the correct class to a very low value so they won't be selected\n",
    "    f_j = torch.where(mask, f, torch.tensor(-1e10).to(f.device)).max(dim=1).values\n",
    "    \n",
    "    # Compute the loss and apply the ReLU operation\n",
    "    loss = torch.relu(f_yi - f_j)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def RandGradEst(x, y_true, v, d, model):\n",
    "    \"\"\"\n",
    "    Two-point (gaussian) random gradient estimator\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    - model: the PyTorch model used for prediction inside the loss function F_torch\n",
    "\n",
    "    Returns:\n",
    "    - Gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "\n",
    "    # Create a tensor with standard normal values\n",
    "    u = torch.randn((1, d, d), device=device)\n",
    "\n",
    "    # Compute the F function values for the two points\n",
    "    F_plus = F_torch(x + v*u, y_true, model)\n",
    "    F_ = F_torch(x, y_true, model)\n",
    "\n",
    "    # Calculate the gradient estimate\n",
    "    grad_estimate = (d/v)*(F_plus - F_)*u\n",
    "\n",
    "    return grad_estimate\n",
    "\n",
    "def Avg_RandGradEst(x, y_true, q, v, d, model):\n",
    "    \"\"\"\n",
    "    Averaged (gaussian) random gradient estimator\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - q: number of random directions\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    - model: the PyTorch model used for prediction inside the F_torch function\n",
    "\n",
    "    Returns:\n",
    "    - Averaged gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "\n",
    "    # Create a tensor with standard normal values for all q directions\n",
    "    u = torch.randn((q, d, d), device=device)\n",
    "\n",
    "    # Compute the F function value for the original point\n",
    "    F_ = F_torch(x, y_true, model)\n",
    "    \n",
    "    g = 0\n",
    "    for j in range(q):\n",
    "        F_plus = F_torch(x + v*u[j], y_true, model)\n",
    "        g = g + (F_plus - F_)*u[j]\n",
    "\n",
    "    # Calculate the averaged gradient estimate\n",
    "    avg_grad_estimate = (d/(v*q))*g\n",
    "\n",
    "    return avg_grad_estimate\n",
    "\n",
    "def stop_attack(x, y_true, model):\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model outputs raw logits\n",
    "        logits = model(x)\n",
    "        success = torch.argmax(logits, dim=1)\n",
    "        return torch.sum(success == y_true).item() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_Par_torch(x, y_true, model):\n",
    "    \"\"\"\n",
    "    Loss function for only one example\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true label of the image [batch_size]\n",
    "\n",
    "    Returns:\n",
    "    - Computed loss values\n",
    "    \"\"\"\n",
    "    # Model prediction\n",
    "    f = model(x)\n",
    "    \n",
    "    # Gather the predictions for the true class label\n",
    "    f_yi = torch.gather(f, 1, y_true.view(-1, 1)).squeeze()\n",
    "    \n",
    "    # Mask out the true class labels\n",
    "    f_j_values, _ = torch.max(f + (y_true.view(-1, 1) == 1).float() * -1e10, dim=1)\n",
    "\n",
    "    return torch.clamp(f_yi - f_j_values, min=0)\n",
    "\n",
    "def Avg_RandGradEst_Par(x, y_true, q, v, d, model):\n",
    "    \"\"\"\n",
    "    Averaged (gaussian) random gradient estimator in parallel\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - q: number of random directions\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    \n",
    "    Returns:\n",
    "    - Averaged gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "    \n",
    "    # Generate random directions\n",
    "    u = torch.randn((q, d), device=device)\n",
    "\n",
    "    # Compute the F function value for the original point\n",
    "    F_ = F_Par_torch(x, y_true, model).mean()\n",
    "\n",
    "    # Create tensors for q directions and compute the difference\n",
    "    x_par_plus = (x.unsqueeze(0) + v*u.unsqueeze(1)).view(-1, *x.shape[1:])\n",
    "    diff = F_Par_torch(x_par_plus, y_true.repeat(q), model) - F_\n",
    "\n",
    "    # Compute the gradient estimate\n",
    "    g = torch.sum((diff.view(q, -1) / v).unsqueeze(2) * u.unsqueeze(1), dim=0)\n",
    "\n",
    "    return (d/(q*v))*g.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an unbatched DataLoader\n",
    "test_grabber = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def get_data(n, c, test_loader):\n",
    "    \"\"\"\n",
    "    Return x, x_ori, y_true_in.\n",
    "    \"\"\"\n",
    "    x_ori, y_true_in = extract_images(n, c, test_loader)\n",
    "    x = x_ori.clone()\n",
    "    return x, x_ori, y_true_in\n",
    "\n",
    "def extract_images(n, c, test_loader):\n",
    "    \"\"\"\n",
    "    Extract some images of the same class from a DataLoader.\n",
    "\n",
    "    Input:\n",
    "    - n: number of images to extract\n",
    "    - c: label\n",
    "    - test_loader: DataLoader to extract images from\n",
    "    \"\"\"\n",
    "    x_extr = []\n",
    "    y_extr = []\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        if y.item() == c:\n",
    "            x_extr.append(x)\n",
    "            y_extr.append(y)\n",
    "            \n",
    "        if len(x_extr) == n:\n",
    "            break\n",
    "\n",
    "    return torch.stack(x_extr).view((n,1,32,32)), torch.stack(y_extr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ZOFW(N, d, s, m_k, x, y_true_in, model, v=-1, alpha=-1, B=1, verbose=True, clip=False):\n",
    "    device = x.device  # Assuming x is already a PyTorch tensor\n",
    "\n",
    "    if v == -1:\n",
    "        v = sqrt(2 / (N * (d + 3)**3))\n",
    "    if alpha == -1:\n",
    "        alpha = 1 / torch.sqrt(torch.tensor(N, dtype=torch.float32))\n",
    "\n",
    "    x_ori = x.clone()\n",
    "    loss_ZSCG = []\n",
    "    perturbations = []\n",
    "    loss_ZSCG.append(F_torch(x, y_true_in, model))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Epoch:\", 0, \"Loss:\", F_torch(x_ori, y_true_in, model).item(), \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "    for k in range(N):\n",
    "        v_k = 0\n",
    "        for i in tqdm(range(x.shape[0]), disable= not verbose):\n",
    "            v_k += Avg_RandGradEst(x[i:i+1], y_true_in[i:i+1], m_k, v, 32, model)\n",
    "            #v_k += Avg_RandGradEst_Par(x[i:i+1], y_true_in[i:i+1], m_k, v, d, model)\n",
    "\n",
    "        v_k = (1 / x.shape[0]) * v_k\n",
    "\n",
    "        x_k = -s * torch.sign(v_k) + x_ori \n",
    "        x = (1 - alpha) * x + alpha * x_k\n",
    "        if clip:\n",
    "            x = x_ori + torch.clamp((x - x_ori), 0, 1)\n",
    "        perturbations.append(x)\n",
    "        loss_ZSCG.append(F_torch(x, y_true_in, model).item())\n",
    "        if verbose:\n",
    "            print(\"-\"*100)\n",
    "            print(\"Epoch:\", k+1, \"Loss:\", loss_ZSCG[k], \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "        if stop_attack(x, y_true_in, model):  # Assuming this function handles PyTorch tensors\n",
    "            print(\"Attack successful! stopping computation...\")\n",
    "            return loss_ZSCG, x\n",
    "\n",
    "    ZSCG_x_perturbated = x\n",
    "    print(\"ZSCG Final loss =\", loss_ZSCG[-1])\n",
    "    return loss_ZSCG, ZSCG_x_perturbated, perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100 # EPOCHS\n",
    "n = 100 # number of examples to attack\n",
    "d = 32*32 # dimension\n",
    "q = 30 # number of times to sample gradient\n",
    " # samples to run on [b,1,d] NOTE: this is flattened\n",
    "x, _, y_true = get_data(n, 4,test_grabber)\n",
    "s = 0.1 # step size\n",
    "#loss_Z, x_Z, p1=SZOFW(T, d, 0.1, q, x, y_true, LeNet, verbose=True)\n",
    "#ZSCG(T, d, 0.1, q, x, y_true_in, LeNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5, 31.5, 31.5, -0.5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1,3)\n",
    "axs[0].imshow(a[-1][0].squeeze().detach().numpy(),cmap='gray')\n",
    "axs[0].set_title(l[-1].item())\n",
    "axs[0].axis('off')\n",
    "axs[1].imshow(a[-2][0].squeeze().detach().numpy(),cmap='gray')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title(l[-2].item())\n",
    "axs[2].imshow(a[-3][0].squeeze().detach().numpy(),cmap='gray')\n",
    "axs[2].set_title(l[-3].item())\n",
    "axs[2].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CoordGradEst_torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\flat_ref SZOFW.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(l)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m#print(flatten_data(a).shape)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m#print(\"loss test\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m#print(F_torch(a,l,LeNet))\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m#print(\"grad test\")\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m#print(RandGradEst(a,l,0.3,32,LeNet))\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m#print(Avg_RandGradEst(a,l,30,0.3,32,LeNet).shape)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(CoordGradEst_torch(a[\u001b[39m0\u001b[39m], l[\u001b[39m0\u001b[39m], \u001b[39m0.3\u001b[39m, \u001b[39m32\u001b[39m, LeNet))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m#print('Avg_RandGradEst_Par')\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m#print(Avg_RandGradEst_Par(a, l, 30, 0.3, 32, LeNet).shape)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CoordGradEst_torch' is not defined"
     ]
    }
   ],
   "source": [
    "out = True\n",
    "for a,l in test_loader:\n",
    "    if out:\n",
    "        print(l)\n",
    "        #print(flatten_data(a).shape)\n",
    "        #print(\"loss test\")\n",
    "        #print(F_torch(a,l,LeNet))\n",
    "        #print(\"grad test\")\n",
    "        #print(RandGradEst(a,l,0.3,32,LeNet))\n",
    "        #print(Avg_RandGradEst(a,l,30,0.3,32,LeNet).shape)\n",
    "        print(CoordGradEst_torch(a[0], l[0], 0.3, 32, LeNet))\n",
    "        #print('Avg_RandGradEst_Par')\n",
    "        #print(Avg_RandGradEst_Par(a, l, 30, 0.3, 32, LeNet).shape)\n",
    "    out = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 34 (3790393094.py, line 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 36\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 34\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "def ACC_ZOFW(T, d, s, q,n, x, y_true_in, model, v=-1, alpha=-1, B=1, verbose=True, clip=False, GE = \"uni\"):\n",
    "    \"\"\"\"\n",
    "    Employs a varience reduction technique to reduce the query capacity\n",
    "    T, int: Number of Epochs\n",
    "    d, int: dim along one side of example. Shape: (d, d)\n",
    "    q, int: mini-batch size\n",
    "    x, input data of shape ()\n",
    "    \"\"\"\n",
    "    device = x.device  # Assuming x is already a PyTorch tensor\n",
    "    if GE == \"uni\": \n",
    "        grad_est = Avg_RandGradEst\n",
    "    elif GE == \"coo\":\n",
    "        grad_est = Avg_RandGradEst\n",
    "    if v == -1:\n",
    "        v = sqrt(2 / (T * (d + 3)**3))\n",
    "    if alpha == -1:\n",
    "        alpha = 1 / torch.sqrt(torch.tensor(T, dtype=torch.float32))\n",
    "\n",
    "    x_ori = x.clone()\n",
    "    loss_ZSCG = []\n",
    "    perturbations = []\n",
    "    loss_ZSCG.append(F_torch(x, y_true_in, model))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Epoch:\", 0, \"Loss:\", F_torch(x_ori, y_true_in, model).item(), \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "    for t in range(T):\n",
    "        v_t = 0\n",
    "        if mod(t,q) == 0:\n",
    "            for i in range(b1):\n",
    "                v_t = v_t + grad_est()\n",
    "            v_t = v_t / b1\n",
    "        else:\n",
    "            #for i in np.random.rand(0,x.shape[0], b2):\n",
    "            for i in range(0, ): \n",
    "                \n",
    "        '''\n",
    "        for i in tqdm(range(x.shape[0]), disable= not verbose):\n",
    "            v_k += Avg_RandGradEst(x[i:i+1], y_true_in[i:i+1], m_k, v, 32, model)\n",
    "            #v_k += Avg_RandGradEst_Par(x[i:i+1], y_true_in[i:i+1], m_k, v, d, model)\n",
    "        '''\n",
    "        v_t = (1 / x.shape[0]) * v_t\n",
    "\n",
    "        x_t = -s * torch.sign(v_t) + x_ori \n",
    "        x = (1 - alpha) * x + alpha * x_t\n",
    "        if clip:\n",
    "            x = x_ori + torch.clamp((x - x_ori), 0, 1)\n",
    "        perturbations.append(x)\n",
    "        loss_ZSCG.append(F_torch(x, y_true_in, model).item())\n",
    "        if verbose:\n",
    "            print(\"-\"*100)\n",
    "            print(\"Epoch:\", t+1, \"Loss:\", loss_ZSCG[t], \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "        if stop_attack(x, y_true_in, model):  # Assuming this function handles PyTorch tensors\n",
    "            print(\"Attack successful! stopping computation...\")\n",
    "            return loss_ZSCG, x\n",
    "\n",
    "    ZSCG_x_perturbated = x\n",
    "    print(\"ZSCG Final loss =\", loss_ZSCG[-1])\n",
    "    return loss_ZSCG, ZSCG_x_perturbated, perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e(i, d, device):\n",
    "    \"\"\"\n",
    "    Orthogonal basis vector in PyTorch\n",
    "\n",
    "    Input:\n",
    "    - i: index\n",
    "    - d: dimensions\n",
    "    - device: the device (CPU or CUDA) to place the tensor on\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor of shape [1, d, d]\n",
    "    \"\"\"\n",
    "    ei = torch.zeros(1, d, d, device=device)\n",
    "    ei[0, i//d, i%d] = 1\n",
    "    return ei\n",
    "\n",
    "def CoordGradEst_Par(x, y_true, mu, d, model):\n",
    "    \"\"\"\n",
    "    Coordinate-wise gradient estimator in parallel using PyTorch.\n",
    "\n",
    "    Input:\n",
    "    - x: image of shape [1, d, d]\n",
    "    - y_true: true scalar label of the image\n",
    "    - mu: smoothing parameter\n",
    "    - model: PyTorch model used for predictions\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device (CPU or CUDA) of the tensor\n",
    "\n",
    "    # Generate x_par_plus and x_par_minus using list comprehension\n",
    "    x_par_plus = torch.stack([x + mu*e(j, d, device) for j in range(d*d)])\n",
    "    x_par_minus = torch.stack([x - mu*e(j, d, device) for j in range(d*d)])\n",
    "\n",
    "    diff = F_torch(x_par_plus, y_true, model) - F_torch(x_par_minus, y_true, model)\n",
    "    print(x_par_plus.shape)\n",
    "\n",
    "    q = 0\n",
    "    for j in range(d):\n",
    "        q = q + diff[j] * e(j, d, device)\n",
    "\n",
    "    return (1 / (2 * mu)) * q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def F_Par_torch(x, y_true, model):\n",
    "    \"\"\"\n",
    "    PyTorch version of the F_Par loss function for only one example.\n",
    "\n",
    "    Input:\n",
    "    - x: PyTorch tensor of the image of shape [batch_size, 1, d, d]\n",
    "    - y_true: PyTorch tensor of true labels of the images of shape [batch_size]\n",
    "    - model: a PyTorch model for predictions\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor of the calculated losses for each image in the batch\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "\n",
    "    # Getting model predictions\n",
    "    logits = model(x)\n",
    "    \n",
    "    # Calculate f_yi\n",
    "    # Using gather here to extract the scores at the true class indices\n",
    "    f_yi = logits.gather(1, y_true.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Calculate f_j\n",
    "    # Creating a mask to set true class logits to a very small value so they're not selected in the max operation\n",
    "    mask = torch.ones_like(logits) * 1e10\n",
    "    mask.scatter_(1, y_true.view(-1, 1), -1e10)\n",
    "    f_j, _ = torch.max(logits - mask, dim=1)\n",
    "\n",
    "    # Compute the final loss values per example\n",
    "    losses = torch.where(f_yi-f_j > 0, f_yi-f_j, torch.tensor(0.0, device=device))\n",
    "    \n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CoordGradEst_torch(x, y_true, mu, d, model):\n",
    "    \"\"\"\n",
    "    PyTorch version of the coordinate-wise gradient estimator.\n",
    "\n",
    "    Input:\n",
    "    - x: PyTorch tensor of the images of shape [batch_size, 1, d, d]\n",
    "    - y_true: PyTorch tensor of the true labels of the images of shape [batch_size]\n",
    "    - mu: smoothing parameter\n",
    "    - d: dimensions of the image (typically height x width for a grayscale image)\n",
    "    - model: PyTorch model for predictions inside the F_Par_torch function (or your appropriate loss function)\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor of the coordinate-wise gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "    \n",
    "    q = torch.zeros(d, d, device=device)  # Assuming d is the height x width of the image\n",
    "\n",
    "    for j in range(d*d):\n",
    "        F_plus = F_torch(x + mu * e(j, d,device).view(1, 1, d, d), y_true, model)\n",
    "        F_minus = F_torch(x - mu * e(j, d,device).view(1, 1, d, d), y_true, model)\n",
    "        diff = F_plus - F_minus\n",
    "        q = q + diff * e(j, d, device)\n",
    "\n",
    "    return q / (2 * mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST \n",
    "def CoordGradEst_torch_optimized(x, y_true, mu, d, model):\n",
    "    \"\"\"\n",
    "    Optimized PyTorch version of the coordinate-wise gradient estimator.\n",
    "\n",
    "    Input:\n",
    "    - x: PyTorch tensor of the images of shape [1, d, d]\n",
    "    - y_true: PyTorch tensor of the true labels of the images of shape [1]\n",
    "    - mu: smoothing parameter\n",
    "    - d: dimensions of the image (typically height x width for a grayscale image)\n",
    "    - model: PyTorch model for predictions\n",
    "\n",
    "    Returns:\n",
    "    - PyTorch tensor of the coordinate-wise gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "    \n",
    "    # Create tensors for all perturbations\n",
    "    basis_vectors = [e(j, d, device) for j in range(d*d)]\n",
    "    x_plus = torch.cat([x + mu * vec for vec in basis_vectors], dim=0)  # [d*d, d, d]\n",
    "    x_minus = torch.cat([x - mu * vec for vec in basis_vectors], dim=0) # [d*d, d, d]\n",
    "    \n",
    "    # Compute model outputs in a batched manner\n",
    "    F_plus = F_torch(x_plus, y_true.expand(d*d), model) # Expanding y_true to match the batch size\n",
    "    F_minus = F_torch(x_minus, y_true.expand(d*d), model)\n",
    "    \n",
    "    diff = F_plus - F_minus\n",
    "    q = (diff.view(d, d) * torch.tensor(basis_vectors, device=device).sum(dim=0)).sum(dim=0)\n",
    "\n",
    "    return q / (2 * mu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 32])\n",
      "tensor(16.7083, grad_fn=<MeanBackward0>)\n",
      "tensor([[[-0.0011, -0.0011, -0.0092,  ..., -0.0028,  0.0000,  0.0000],\n",
      "         [-0.0015, -0.0017, -0.0146,  ..., -0.0038,  0.0000,  0.0000],\n",
      "         [-0.0019, -0.0026, -0.0305,  ...,  0.0123,  0.0010,  0.0009],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000, -0.0625,  ..., -0.0066,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0556,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000, -0.0290,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CoordGradEst_torch_optimized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\flat_ref SZOFW.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m old_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m old_time\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m new_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(CoordGradEst_torch_optimized(a[\u001b[39m0\u001b[39m],l[\u001b[39m0\u001b[39m],\u001b[39m0.6\u001b[39m,\u001b[39m32\u001b[39m,LeNet))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m new_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m new_time\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X26sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mold:\u001b[39m\u001b[39m\"\u001b[39m, old_time)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CoordGradEst_torch_optimized' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "out = True\n",
    "for a,l in test_loader:\n",
    "    if out:\n",
    "        print(a[0].shape)\n",
    "        print(F_torch(a[0:1],l[0:1], LeNet))\n",
    "        #print(F_Par_torch(a[0:1],l[0:1], LeNet))\n",
    "        old_time = time.time()\n",
    "        print(CoordGradEst_torch(a[0],l[0],0.6,32,LeNet))\n",
    "        old_time = time.time() - old_time\n",
    "        new_time = time.time()\n",
    "        print(CoordGradEst_torch_optimized(a[0],l[0],0.6,32,LeNet))\n",
    "        new_time = time.time() - new_time\n",
    "        print(\"old:\", old_time)\n",
    "        print(\"new\", new_time)\n",
    "        out = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: tensor(12.8952, grad_fn=<MeanBackward0>) Distortion: tensor(0.)\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:40<00:00,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "round() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, *, int decimals, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\flat_ref SZOFW.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m gamma \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m mu \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss_Z, x_Z, p1\u001b[39m=\u001b[39mACC_ZOFW(T, d, n, s, gamma, mu, x, y_true, LeNet, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\flat_ref SZOFW.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch:\u001b[39m\u001b[39m\"\u001b[39m, t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLoss:\u001b[39m\u001b[39m\"\u001b[39m, loss_FZCGS[t], \u001b[39m\"\u001b[39m\u001b[39mDistortion:\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mround(torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39mabs(x\u001b[39m-\u001b[39mx0)),\u001b[39m5\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mif\u001b[39;00m stop_attack(x, y_true_in):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/flat_ref%20SZOFW.ipynb#X34sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mAttack successful! stopping computation...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: round() received an invalid combination of arguments - got (Tensor, int), but expected one of:\n * (Tensor input, *, Tensor out)\n * (Tensor input, *, int decimals, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "#FZFW(epochs, 784,n, 0.1,-1,-1 ,x, y_true_in, verbose=True)\n",
    "#FZFW(K,d,n,s,gamma, mu,x,y_true_in, verbose=True, clip=False)\n",
    "T = 100\n",
    "d = 32*32\n",
    "n = 100\n",
    "s = 0.1\n",
    "gamma = -1\n",
    "mu = -1\n",
    "loss_Z, x_Z, p1=ACC_ZOFW(T, d, n, s, gamma, mu, x, y_true, LeNet, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC_ZOFW(T,d,n,s,gamma, mu, x,y_true_in, model, verbose=True, clip=False):\n",
    "  # b1, b2 with b1 >? b2\n",
    "  # T, int: number of epoch\n",
    "\n",
    "  b1 = n\n",
    "  q = b2 = int(sqrt(n))\n",
    "  if gamma==-1:\n",
    "    gamma = 1/sqrt(T)\n",
    "  if mu==-1:\n",
    "    mu = 1/sqrt(d*T)\n",
    "  \n",
    "  x0 = x.clone()\n",
    "\n",
    "\n",
    "  loss_FZCGS = []\n",
    "  perturbations=[]\n",
    "  loss_FZCGS.append(F_torch(x0, y_true_in, model))\n",
    "  if verbose:\n",
    "    print(\"Epoch:\", 0, \"Loss:\", F_torch(x0, y_true_in, model), \"Distortion:\", torch.max(torch.abs(x-x0)))\n",
    "    print(\"-\"*100)\n",
    "\n",
    "  for t in range(T):\n",
    "\n",
    "    if (t % q == 0):\n",
    "      # CASE 1: GRADIENT ESTIMATION WITH LARGER NUMBER OF SAMPLES\n",
    "      # b1 > b2\n",
    "\n",
    "      v_t = 0\n",
    "      for i in tqdm(range(b1), disable=not verbose):\n",
    "        v_t = v_t + CoordGradEst_torch(x[i:i+1], y_true_in[i:i+1], mu, 32, model)\n",
    "      v_t=v_t/b1\n",
    "      v_t_1 = v_t\n",
    "\n",
    "    else:\n",
    "      # CASE 2: FAST GRADIENT ESTIMATION WITH b2 < b1\n",
    "\n",
    "      v_t = 0\n",
    "      s2_idx = np.random.randint(0, n, b2)\n",
    "\n",
    "      for idx in tqdm(s2_idx,  disable= not verbose):\n",
    "        v_t = v_t + CoordGradEst_torch(x[idx:idx+1], y_true_in[idx:idx+1], mu, 32, model) - CoordGradEst_torch(x_t_1[idx:idx+1], y_true_in[idx:idx+1], mu, 32, model) + v_t_1\n",
    "      v_t = (1/b2) * v_t\n",
    "      v_t_1 = v_t\n",
    "\n",
    "\n",
    "    x_t_1 = x.clone()\n",
    "    # LMO STEP\n",
    "    u_t = -s * torch.sign(v_t) + x0 # Solve the LMO\n",
    "    d_t = u_t - x\n",
    "    x = x + gamma*d_t\n",
    "\n",
    "    if clip:\n",
    "      x= x0 + torch.clip((x-x0), 0, 1)\n",
    "    perturbations.append(x)\n",
    "    loss_FZCGS.append(F_torch(x, y_true_in, model))\n",
    "    if verbose:\n",
    "      print(\"-\"*100)\n",
    "      print(\"Epoch:\", t+1, \"Loss:\", loss_FZCGS[t], \"Distortion:\", torch.round(torch.max(torch.abs(x-x0)),5))\n",
    "    if stop_attack(x, y_true_in):\n",
    "      print(\"Attack successful! stopping computation...\")\n",
    "      return loss_FZCGS, x\n",
    "\n",
    "\n",
    "\n",
    "  FZCGS_x_perturbated = x\n",
    "\n",
    "  print(\"FZCGS Final loss = \", loss_FZCGS[-1])\n",
    "\n",
    "  return loss_FZCGS, FZCGS_x_perturbated, perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ACC_ZOFW_uni(T,d,n,s,gamma, mu, x,y_true_in, model, verbose=True, clip=False):\n",
    "  # b1, b2 with b1 >? b2\n",
    "  # T, int: number of epoch\n",
    "\n",
    "  b1 = n\n",
    "  q = b2 = int(sqrt(n))\n",
    "  if gamma==-1:\n",
    "    gamma = 1/sqrt(T)\n",
    "  if mu==-1:\n",
    "    mu = 1/sqrt(d*T)\n",
    "  v = sqrt(2 / (T * ()))\n",
    "  \n",
    "  x0 = x.clone()\n",
    "\n",
    "\n",
    "  loss_FZCGS = []\n",
    "  perturbations=[]\n",
    "  loss_FZCGS.append(F_torch(x0, y_true_in, model))\n",
    "  if verbose:\n",
    "    print(\"Epoch:\", 0, \"Loss:\", F_torch(x0, y_true_in, model), \"Distortion:\", torch.max(torch.abs(x-x0)))\n",
    "    print(\"-\"*100)\n",
    "\n",
    "  for t in range(T):\n",
    "\n",
    "    if (t % q == 0):\n",
    "      # CASE 1: GRADIENT ESTIMATION WITH LARGER NUMBER OF SAMPLES\n",
    "      # b1 > b2\n",
    "\n",
    "      v_t = 0\n",
    "      for i in tqdm(range(b1), disable=not verbose):\n",
    "        v_t = v_t + CoordGradEst_torch(x[i:i+1], y_true_in[i:i+1], mu, 32, model)\n",
    "      v_t=v_t/b1\n",
    "      v_t_1 = v_t\n",
    "\n",
    "    else:\n",
    "      # CASE 2: FAST GRADIENT ESTIMATION WITH b2 < b1\n",
    "\n",
    "      v_t = 0\n",
    "      s2_idx = np.random.randint(0, n, b2)\n",
    "\n",
    "      for idx in tqdm(s2_idx,  disable= not verbose):\n",
    "        v_t = v_t + CoordGradEst_torch(x[idx:idx+1], y_true_in[idx:idx+1], mu, 32, model) - CoordGradEst_torch(x_t_1[idx:idx+1], y_true_in[idx:idx+1], mu, 32, model) + v_t_1\n",
    "      v_t = (1/b2) * v_t\n",
    "      v_t_1 = v_t\n",
    "\n",
    "\n",
    "    x_t_1 = x.clone()\n",
    "    # LMO STEP\n",
    "    u_t = -s * torch.sign(v_t) + x0 # Solve the LMO\n",
    "    d_t = u_t - x\n",
    "    x = x + gamma*d_t\n",
    "\n",
    "    if clip:\n",
    "      x= x0 + torch.clip((x-x0), 0, 1)\n",
    "    perturbations.append(x)\n",
    "    loss_FZCGS.append(F_torch(x, y_true_in, model))\n",
    "    if verbose:\n",
    "      print(\"-\"*100)\n",
    "      print(\"Epoch:\", t+1, \"Loss:\", loss_FZCGS[t], \"Distortion:\", torch.round(torch.max(torch.abs(x-x0)),5))\n",
    "    if stop_attack(x, y_true_in):\n",
    "      print(\"Attack successful! stopping computation...\")\n",
    "      return loss_FZCGS, x\n",
    "\n",
    "\n",
    "\n",
    "  FZCGS_x_perturbated = x\n",
    "\n",
    "  print(\"FZCGS Final loss = \", loss_FZCGS[-1])\n",
    "\n",
    "  return loss_FZCGS, FZCGS_x_perturbated, perturbations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
