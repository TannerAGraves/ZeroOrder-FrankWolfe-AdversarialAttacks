{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "learning_rate = 10e-3\n",
    "num_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataset and preprocessing\n",
    "train_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                           train = True,\n",
    "                                           transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "                                           download = True)\n",
    "\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root = './data',\n",
    "                                          train = False,\n",
    "                                          transform = transforms.Compose([\n",
    "                                                  transforms.Resize((32,32)),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean = (0.1325,), std = (0.3105,))]),\n",
    "                                          download=True)\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle = False)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                           batch_size = 64, # TEST BATCH SIZE\n",
    "                                           shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the convolutional neural network\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
    "        self.fc = nn.Linear(400, 120)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet = LeNet5(num_classes).to(device)\n",
    "\n",
    "#Setting the loss function\n",
    "cost = nn.CrossEntropyLoss()\n",
    "\n",
    "#Setting the optimizer with the model parameters and learning rate\n",
    "optimizer = torch.optim.Adam(LeNet.parameters(), lr=learning_rate)\n",
    "\n",
    "#this is defined to print how many steps are remaining when training\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet5(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (fc2): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LOAD FROM SAVED FILE\n",
    "LeNet = torch.load('LeNet.pth')\n",
    "LeNet.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://federicozanotti.github.io/2021-09-16-frank-wolfe/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(batched_data):\n",
    "    b, _, d, _ = batched_data.shape\n",
    "    return batched_data.view(b, 1, d*d)\n",
    "\n",
    "def unflatten_data(flattened_data, d):\n",
    "    b, _ = flattened_data.shape[:2]\n",
    "    return flattened_data.view(b, 1, d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4, 1, 4, 9, 5, 9, 0, 6, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 4, 7, 2, 7, 1, 2, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 9, 3])\n",
      "torch.Size([32, 32])\n",
      "Avg_RandGradEst_Par\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1920) must match the size of tensor b (30) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\ref SZOFW.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mprint\u001b[39m(Avg_RandGradEst(a,l,\u001b[39m30\u001b[39m,\u001b[39m0.3\u001b[39m,\u001b[39m32\u001b[39m,LeNet)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAvg_RandGradEst_Par\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mprint\u001b[39m(Avg_RandGradEst_Par(a, l, \u001b[39m30\u001b[39m, \u001b[39m0.3\u001b[39m, \u001b[39m32\u001b[39m, LeNet)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\ref SZOFW.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Compute the F function value for the original point\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m F_ \u001b[39m=\u001b[39m F_torch(x, y_true, model)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m F_plus \u001b[39m=\u001b[39m F_torch(x_q \u001b[39m+\u001b[39m v\u001b[39m*\u001b[39mu, y_q, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m diff \u001b[39m=\u001b[39m F_plus \u001b[39m-\u001b[39m F_\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X10sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m g \u001b[39m=\u001b[39m (diff[:, \u001b[39mNone\u001b[39;00m, :, :] \u001b[39m*\u001b[39m u)\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)  \u001b[39m# Average over the q directions\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1920) must match the size of tensor b (30) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "out = True\n",
    "for a,l in test_loader:\n",
    "    if out:\n",
    "        print(l)\n",
    "        #print(\"loss test\")\n",
    "        #print(F_torch(a,l,LeNet))\n",
    "        #print(\"grad test\")\n",
    "        #print(RandGradEst(a,l,0.3,32,LeNet))\n",
    "        print(Avg_RandGradEst(a,l,30,0.3,32,LeNet).shape)\n",
    "        print('Avg_RandGradEst_Par')\n",
    "        print(Avg_RandGradEst_Par(a, l, 30, 0.3, 32, LeNet).shape)\n",
    "    out = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_torch(x, y_true, model):\n",
    "    \"\"\"\n",
    "    Loss function for all the examples\n",
    "\n",
    "    Input:\n",
    "    - x: images [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - model: the PyTorch model used for prediction\n",
    "\n",
    "    Returns:\n",
    "    - Loss: computed loss\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict class scores\n",
    "    f = model(x)\n",
    "    \n",
    "    # Gather the scores for the correct class\n",
    "    f_yi = torch.gather(f, 1, y_true.view(-1, 1)).squeeze()\n",
    "    \n",
    "    # Create a mask for the correct class, setting them to a very small value\n",
    "    mask = torch.arange(f.size(1)).expand(f.size(0), f.size(1)).to(y_true.device)  # This creates a matrix of size batch_size x num_classes\n",
    "    mask = mask != y_true.view(-1, 1)  # This gives a mask where the correct class for each example is set to False\n",
    "    \n",
    "    # Using the mask, set scores of the correct class to a very low value so they won't be selected\n",
    "    f_j = torch.where(mask, f, torch.tensor(-1e10).to(f.device)).max(dim=1).values\n",
    "    \n",
    "    # Compute the loss and apply the ReLU operation\n",
    "    loss = torch.relu(f_yi - f_j)\n",
    "    \n",
    "    return loss.mean()\n",
    "\n",
    "def RandGradEst(x, y_true, v, d, model):\n",
    "    \"\"\"\n",
    "    Two-point (gaussian) random gradient estimator\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    - model: the PyTorch model used for prediction inside the loss function F_torch\n",
    "\n",
    "    Returns:\n",
    "    - Gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "\n",
    "    # Create a tensor with standard normal values\n",
    "    u = torch.randn((1, d, d), device=device)\n",
    "\n",
    "    # Compute the F function values for the two points\n",
    "    F_plus = F_torch(x + v*u, y_true, model)\n",
    "    F_ = F_torch(x, y_true, model)\n",
    "\n",
    "    # Calculate the gradient estimate\n",
    "    grad_estimate = (d/v)*(F_plus - F_)*u\n",
    "\n",
    "    return grad_estimate\n",
    "\n",
    "def Avg_RandGradEst(x, y_true, q, v, d, model):\n",
    "    \"\"\"\n",
    "    Averaged (gaussian) random gradient estimator\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - q: number of random directions\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    - model: the PyTorch model used for prediction inside the F_torch function\n",
    "\n",
    "    Returns:\n",
    "    - Averaged gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "\n",
    "    # Create a tensor with standard normal values for all q directions\n",
    "    u = torch.randn((q, d, d), device=device)\n",
    "\n",
    "    # Compute the F function value for the original point\n",
    "    F_ = F_torch(x, y_true, model)\n",
    "    \n",
    "    g = 0\n",
    "    for j in range(q):\n",
    "        F_plus = F_torch(x + v*u[j], y_true, model)\n",
    "        g = g + (F_plus - F_)*u[j]\n",
    "\n",
    "    # Calculate the averaged gradient estimate\n",
    "    avg_grad_estimate = (d/(v*q))*g\n",
    "\n",
    "    return avg_grad_estimate\n",
    "\n",
    "def stop_attack(x, y_true, model):\n",
    "    with torch.no_grad():\n",
    "        # Assuming the model outputs raw logits\n",
    "        logits = model(x)\n",
    "        success = torch.argmax(logits, dim=1)\n",
    "        return torch.sum(success == y_true).item() == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_Par_torch(x, y_true, model):\n",
    "    \"\"\"\n",
    "    Loss function for only one example\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true label of the image [batch_size]\n",
    "\n",
    "    Returns:\n",
    "    - Computed loss values\n",
    "    \"\"\"\n",
    "    # Model prediction\n",
    "    f = model(x)\n",
    "    \n",
    "    # Gather the predictions for the true class label\n",
    "    f_yi = torch.gather(f, 1, y_true.view(-1, 1)).squeeze()\n",
    "    \n",
    "    # Mask out the true class labels\n",
    "    f_j_values, _ = torch.max(f + (y_true.view(-1, 1) == 1).float() * -1e10, dim=1)\n",
    "\n",
    "    return torch.clamp(f_yi - f_j_values, min=0)\n",
    "\n",
    "def Avg_RandGradEst_Par(x, y_true, q, v, d, model):\n",
    "    \"\"\"\n",
    "    Averaged (gaussian) random gradient estimator in parallel\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - q: number of random directions\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, channels x height x width)\n",
    "    \n",
    "    Returns:\n",
    "    - Averaged gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device  # Get the device of the input tensor (either CPU or CUDA)\n",
    "    \n",
    "    # Generate random directions\n",
    "    u = torch.randn((q, d), device=device)\n",
    "\n",
    "    # Compute the F function value for the original point\n",
    "    F_ = F_Par_torch(x, y_true, model).mean()\n",
    "\n",
    "    # Create tensors for q directions and compute the difference\n",
    "    x_par_plus = (x.unsqueeze(0) + v*u.unsqueeze(1)).view(-1, *x.shape[1:])\n",
    "    diff = F_Par_torch(x_par_plus, y_true.repeat(q), model) - F_\n",
    "\n",
    "    # Compute the gradient estimate\n",
    "    g = torch.sum((diff.view(q, -1) / v).unsqueeze(2) * u.unsqueeze(1), dim=0)\n",
    "\n",
    "    return (d/(q*v))*g.mean(dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avg_RandGradEst_Par(x, y_true, q, v, d, model):\n",
    "    \"\"\"\n",
    "    Averaged (gaussian) random gradient estimator in parallel\n",
    "\n",
    "    Input:\n",
    "    - x: image [batch_size, channels, height, width]\n",
    "    - y_true: true labels of the images [batch_size]\n",
    "    - q: number of random directions\n",
    "    - v: smoothing parameter\n",
    "    - d: dimensionality of the image (typically, height x width)\n",
    "    - model: the PyTorch model used for prediction inside the F_torch function\n",
    "\n",
    "    Returns:\n",
    "    - Averaged gradient estimate\n",
    "    \"\"\"\n",
    "    device = x.device\n",
    "\n",
    "    # Create a tensor with standard normal values for all q directions\n",
    "    u = torch.randn((q, 1, d, d), device=device)\n",
    "\n",
    "    # Duplicate x for all q directions\n",
    "    x_q = x.repeat(q, 1, 1, 1)\n",
    "    y_q = y_true.repeat(q)\n",
    "\n",
    "    # Compute the F function value for the original point\n",
    "    F_ = F_torch(x, y_true, model)\n",
    "    \n",
    "    F_plus = F_torch(x_q + v*u, y_q, model)\n",
    "    diff = F_plus - F_\n",
    "\n",
    "    g = (diff[:, None, :, :] * u).sum(dim=0)  # Average over the q directions\n",
    "\n",
    "    # Calculate the averaged gradient estimate\n",
    "    avg_grad_estimate = (d/(v*q))*g\n",
    "\n",
    "    return avg_grad_estimate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ZSCG(N, d, s, m_k, x, y_true_in, model, v=-1, alpha=-1, B=1, verbose=True, clip=False):\n",
    "    device = x.device  # Assuming x is already a PyTorch tensor\n",
    "\n",
    "    if v == -1:\n",
    "        v = torch.sqrt(2 / (N * (d + 3)**3))\n",
    "    if alpha == -1:\n",
    "        alpha = 1 / torch.sqrt(torch.tensor(N, dtype=torch.float32))\n",
    "\n",
    "    x_ori = x.clone()\n",
    "    loss_ZSCG = []\n",
    "    perturbations = []\n",
    "    loss_ZSCG.append(F_torch(x, y_true_in, model))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Epoch:\", 0, \"Loss:\", F_torch(x_ori, y_true_in, model).item(), \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "    for k in range(N):\n",
    "        v_k = 0\n",
    "        for i in tqdm(range(x.shape[0]), disable= not verbose):\n",
    "            v_k += Avg_RandGradEst(x[i:i+1], y_true_in[i:i+1], m_k, v, d, model)\n",
    "            #v_k += Avg_RandGradEst_Par(x[i:i+1], y_true_in[i:i+1], m_k, v, d, model)\n",
    "\n",
    "        v_k = (1 / x.shape[0]) * v_k\n",
    "\n",
    "        x_k = -s * torch.sign(v_k) + x_ori \n",
    "        x = (1 - alpha) * x + alpha * x_k\n",
    "        if clip:\n",
    "            x = x_ori + torch.clamp((x - x_ori), 0, 1)\n",
    "        perturbations.append(x)\n",
    "        loss_ZSCG.append(F_torch(x, y_true_in, model).item())\n",
    "        if verbose:\n",
    "            print(\"-\"*100)\n",
    "            print(\"Epoch:\", k+1, \"Loss:\", loss_ZSCG[k], \"Distortion:\", torch.max(torch.abs(x - x_ori)).item())\n",
    "        if stop_attack(x, y_true_in):  # Assuming this function handles PyTorch tensors\n",
    "            print(\"Attack successful! stopping computation...\")\n",
    "            return loss_ZSCG, x\n",
    "\n",
    "    ZSCG_x_perturbated = x\n",
    "    print(\"ZSCG Final loss =\", loss_ZSCG[-1])\n",
    "    return loss_ZSCG, ZSCG_x_perturbated, perturbations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "x, _, y_true_in = get_data(n, 4,test_grabber)\n",
    "epochs=100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "28**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ZSCG() missing 1 required positional argument: 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\JIC\\Documents\\ZOFW scratch\\ref SZOFW.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/JIC/Documents/ZOFW%20scratch/ref%20SZOFW.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m loss_Z, x_Z, p1\u001b[39m=\u001b[39mZSCG(epochs, \u001b[39m784\u001b[39m, \u001b[39m0.1\u001b[39m, \u001b[39m30\u001b[39m, x, y_true_in,verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: ZSCG() missing 1 required positional argument: 'model'"
     ]
    }
   ],
   "source": [
    "\n",
    "ZSCG(N, d, s, m_k, x, y_true_in,v=-1,alpha=-1, B=1,verbose=True, clip=False)\n",
    "loss_Z, x_Z, p1=ZSCG(epochs, 784, 0.1, 30, x, y_true_in,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create an unbatched DataLoader\n",
    "test_grabber = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "def get_data(n, c, test_loader):\n",
    "    \"\"\"\n",
    "    Return x, x_ori, y_true_in.\n",
    "    \"\"\"\n",
    "    x_ori, y_true_in = extract_images(n, c, test_loader)\n",
    "    x = x_ori.clone()\n",
    "    return x, x_ori, y_true_in\n",
    "\n",
    "def extract_images(n, c, test_loader):\n",
    "    \"\"\"\n",
    "    Extract some images of the same class from a DataLoader.\n",
    "\n",
    "    Input:\n",
    "    - n: number of images to extract\n",
    "    - c: label\n",
    "    - test_loader: DataLoader to extract images from\n",
    "    \"\"\"\n",
    "    x_extr = []\n",
    "    y_extr = []\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        if y.item() == c:\n",
    "            x_extr.append(x)\n",
    "            y_extr.append(y)\n",
    "            \n",
    "        if len(x_extr) == n:\n",
    "            break\n",
    "\n",
    "    return torch.stack(x_extr), torch.stack(y_extr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
